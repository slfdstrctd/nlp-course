{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c4ccfbb8067a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:38:28.667463700Z",
     "start_time": "2023-10-30T12:38:25.868185Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio torchtext --index-url https: // download.pytorch.org/whl/cu118 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd1bf2e689430d2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:38:31.711534100Z",
     "start_time": "2023-10-30T12:38:28.678466800Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn spacy tensorflow tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting en-core-web-sm==3.7.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m8.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from en-core-web-sm==3.7.0) (3.7.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.2.1)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.3.3)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (6.4.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.64.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.28.1)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (66.1.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (23.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (1.24.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (2.10.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (4.8.0)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.1.3)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/slfdstrctd/.local/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-sm==3.7.0) (0.16.0)\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T12:38:49.353427400Z",
     "start_time": "2023-10-30T12:38:40.309003700Z"
    }
   },
   "id": "2d81722479cb710a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d526b547f144d3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:39:04.793846300Z",
     "start_time": "2023-10-30T12:39:04.787845100Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6c877a75bcdeb21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:40:04.789861600Z",
     "start_time": "2023-10-30T12:40:03.582665800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(44898, 5)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = pd.read_csv('True.csv')\n",
    "fake = pd.read_csv('Fake.csv')\n",
    "\n",
    "fake[\"is_fake\"] = 1\n",
    "true[\"is_fake\"] = 0\n",
    "\n",
    "df = pd.concat([true, fake])\n",
    "del fake, true\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479b5fc84b231226",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:41:44.052251500Z",
     "start_time": "2023-10-30T12:41:38.645631800Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "NUM_WORDS = 10000\n",
    "SENTENCE_LENGTH = 100\n",
    "EMBED_DIM = 1000\n",
    "random_state = 42\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    return text.progress_apply(lambda x: \" \".join(\n",
    "        lemmatizer.lemmatize(token.lower()) for token in word_tokenize(x)\n",
    "        if token.isalnum()\n",
    "        and token not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "733662037d403590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:43:16.723139300Z",
     "start_time": "2023-10-30T12:41:56.551224Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44898/44898 [01:19<00:00, 564.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 984 ms, total: 1min 20s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['cleaned_text'] = df['title'] + \" \" + df['text']\n",
    "df['cleaned_text'] = preprocess_text(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "259949b64fab75b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:44:00.032806500Z",
     "start_time": "2023-10-30T12:44:00.021799200Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = (train_test_split(df['cleaned_text'], df['is_fake'], test_size=0.2,\n",
    "                                                     random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b3be3b96ebababe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:44:07.940272900Z",
     "start_time": "2023-10-30T12:44:01.280114800Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = pad_sequences(train_seq, maxlen=SENTENCE_LENGTH)\n",
    "x_test = pad_sequences(test_seq, maxlen=SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40df0a0b83eb4877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:44:07.965206200Z",
     "start_time": "2023-10-30T12:44:07.939854100Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = torch.tensor(data).to(torch.int64)\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "torch.manual_seed(random_state)\n",
    "train_dataset = SpamDataset(x_train, y_train.to_numpy())\n",
    "test_dataset = SpamDataset(x_test, y_test.to_numpy())\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdff1ed0c2c18a44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:44:07.970816100Z",
     "start_time": "2023-10-30T12:44:07.967266700Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embed = nn.Embedding(NUM_WORDS, embed_dim)\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3)\n",
    "        self.conv2 = nn.Conv2d(1, 1, 3)\n",
    "        self.conv3 = nn.Conv2d(1, 1, 3)\n",
    "        self.fc = nn.Linear(93436, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        out = out.unsqueeze(1)\n",
    "        out = F.relu(self.conv1(out))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b0ac0d25d89018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:44:07.978349400Z",
     "start_time": "2023-10-30T12:44:07.972355300Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, seq_len):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(seq_len * hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = torch.reshape(x, (x.size(0), -1,))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ff5c6f434481e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:44:08.917277300Z",
     "start_time": "2023-10-30T12:44:08.859280600Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, epochs, train_dataloader, test_dataloader, device, criterion, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0\n",
    "            correct_predictions = 0\n",
    "            total = 0\n",
    "            for i, (inputs, targets) in enumerate(tqdm(self.train_dataloader)):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets.long())\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "            train_loss = running_loss / len(train_dataloader)\n",
    "            train_accuracy = correct_predictions / total\n",
    "            print(f'Epoch: {epoch + 1}/{self.epochs}, Loss: {train_loss:.6f}, Train accuracy: {train_accuracy:.6f}')\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.test_dataloader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets.long())\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "        val_loss /= len(self.test_dataloader)\n",
    "        val_accuracy = correct_predictions / len(test_dataset)\n",
    "\n",
    "        print(f'Validation loss: {val_loss:.6f}, Validation accuracy: {val_accuracy:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c6869de049e549b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:44:19.077061400Z",
     "start_time": "2023-10-30T12:44:09.440369500Z"
    }
   },
   "outputs": [],
   "source": [
    "CNN_model = CNN(EMBED_DIM).to(device)\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr=0.001)\n",
    "CNN_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "485cc1c8e5a5323c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:44:19.082053700Z",
     "start_time": "2023-10-30T12:44:19.080060800Z"
    }
   },
   "outputs": [],
   "source": [
    "CNN_trainer = Trainer(CNN_model, 5, train_dataloader, test_dataloader, device, CNN_criterion, CNN_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8b488bdd222f85b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:45:22.410103200Z",
     "start_time": "2023-10-30T12:44:19.082053700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/141 [00:00<?, ?it/s]/home/slfdstrctd/.local/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 141/141 [00:25<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Loss: 3.244441, Train accuracy: 0.533827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:09<00:00, 15.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5, Loss: 0.516596, Train accuracy: 0.794309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:09<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5, Loss: 0.125326, Train accuracy: 0.956039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:09<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5, Loss: 0.060348, Train accuracy: 0.981708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:09<00:00, 14.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5, Loss: 0.025716, Train accuracy: 0.994682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CNN_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcba9d3eafb2cf40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:45:22.859025900Z",
     "start_time": "2023-10-30T12:45:22.427102500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.069707, Validation accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "CNN_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "636f9382b19109da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:45:23.182768300Z",
     "start_time": "2023-10-30T12:45:22.866023800Z"
    }
   },
   "outputs": [],
   "source": [
    "LSTM_model = LSTM(vocab_size=NUM_WORDS, embedding_dim=EMBED_DIM, hidden_dim=100, n_layers=3,\n",
    "                  seq_len=SENTENCE_LENGTH).to(device)\n",
    "LSTM_optimizer = torch.optim.Adam(LSTM_model.parameters(), lr=0.001)\n",
    "LSTM_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f2f785fd6f39615",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:45:23.190778800Z",
     "start_time": "2023-10-30T12:45:23.184773Z"
    }
   },
   "outputs": [],
   "source": [
    "LSTM_trainer = Trainer(LSTM_model, 2, train_dataloader, test_dataloader, device, LSTM_criterion, LSTM_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fd81cb422cd7f40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:45:33.710873Z",
     "start_time": "2023-10-30T12:45:23.190778800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:06<00:00, 20.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2, Loss: 0.180451, Train accuracy: 0.914360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:03<00:00, 39.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2, Loss: 0.027309, Train accuracy: 0.991481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LSTM_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "734709f28314625f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:45:34.026580100Z",
     "start_time": "2023-10-30T12:45:33.712018600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.051842, Validation accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "LSTM_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb9f6c6420c41c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:45:34.026580100Z",
     "start_time": "2023-10-30T12:45:34.026580100Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dfcd77cc16eca64f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
