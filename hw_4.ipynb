{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5c4ccfbb8067a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:08.639158300Z",
     "start_time": "2023-10-30T12:29:07.053142300Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/cu118 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd1bf2e689430d2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:11.407726900Z",
     "start_time": "2023-10-30T12:29:08.674245100Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas nltk scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d526b547f144d3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:17.817005800Z",
     "start_time": "2023-10-30T12:29:11.440928800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2mâœ” Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6c877a75bcdeb21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:28.107555Z",
     "start_time": "2023-10-30T12:29:28.099559Z"
    }
   },
   "outputs": [],
   "source": [
    "true = pd.read_csv('True.csv')\n",
    "fake = pd.read_csv('Fake.csv')\n",
    "\n",
    "fake[\"is_fake\"] = 1\n",
    "true[\"is_fake\"] = 0\n",
    "\n",
    "df = pd.concat([true, fake])\n",
    "del fake, true\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479b5fc84b231226",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T11:59:33.621102800Z",
     "start_time": "2023-10-30T11:59:31.726159700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44898, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "NUM_WORDS = 10000\n",
    "SENTENCE_LENGTH = 100\n",
    "EMBED_DIM = 1000\n",
    "random_state = 42\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.progress_apply(\n",
    "        lambda x: \" \".join(\n",
    "            token.lemma_.lower() for token in nlp(x) if\n",
    "            not token.is_stop\n",
    "            and not token.is_punct\n",
    "            and not token.is_digit\n",
    "            and not token.like_email\n",
    "            and not token.like_num\n",
    "            and not token.is_space\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "733662037d403590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:37.152505500Z",
     "start_time": "2023-10-30T12:29:36.145815200Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df['cleaned_text'] = df['title'] + \" \" + df['text']\n",
    "df['cleaned_text'] = preprocess_text(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259949b64fab75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['is_fake'], test_size=0.2,\n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b3be3b96ebababe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:41.124256400Z",
     "start_time": "2023-10-30T12:29:41.069696100Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = pad_sequences(train_seq, maxlen=SENTENCE_LENGTH)\n",
    "x_test = pad_sequences(test_seq, maxlen=SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "40df0a0b83eb4877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:50.231614200Z",
     "start_time": "2023-10-30T12:29:41.923887400Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = torch.tensor(data).to(torch.int64)\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "torch.manual_seed(random_state)\n",
    "train_dataset = SpamDataset(x_train, y_train.to_numpy())\n",
    "test_dataset = SpamDataset(x_test, y_test.to_numpy())\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fdff1ed0c2c18a44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:50.288283900Z",
     "start_time": "2023-10-30T12:29:50.231614200Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embed = nn.Embedding(NUM_WORDS, embed_dim)\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3)\n",
    "        self.conv2 = nn.Conv2d(1, 1, 3)\n",
    "        self.conv3 = nn.Conv2d(1, 1, 3)\n",
    "        self.fc = nn.Linear(93436, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        out = out.unsqueeze(1)\n",
    "        out = F.relu(self.conv1(out))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88b0ac0d25d89018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:50.294989500Z",
     "start_time": "2023-10-30T12:29:50.292970500Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, seq_len):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(seq_len * hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = torch.reshape(x, (x.size(0), -1,))\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ff5c6f434481e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:50.348820800Z",
     "start_time": "2023-10-30T12:29:50.298328200Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, epochs, train_dataloader, test_dataloader, device, criterion, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0\n",
    "            correct_predictions = 0\n",
    "            total = 0\n",
    "            for i, (inputs, targets) in enumerate(tqdm(self.train_dataloader)):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets.long())\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "            train_loss = running_loss / len(train_dataloader)\n",
    "            train_accuracy = correct_predictions / total\n",
    "            print(f'Epoch: {epoch + 1}/{self.epochs}, Loss: {train_loss:.6f}, Train accuracy: {train_accuracy:.6f}')\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.test_dataloader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets.long())\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "        val_loss /= len(self.test_dataloader)\n",
    "        val_accuracy = correct_predictions / len(test_dataset)\n",
    "\n",
    "        print(f'Validation loss: {val_loss:.6f}, Validation accuracy: {val_accuracy:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8c6869de049e549b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:50.350821700Z",
     "start_time": "2023-10-30T12:29:50.343233Z"
    }
   },
   "outputs": [],
   "source": [
    "CNN_model = CNN(EMBED_DIM).to(device)\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr=0.001)\n",
    "CNN_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "485cc1c8e5a5323c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:50.447227500Z",
     "start_time": "2023-10-30T12:29:50.352831300Z"
    }
   },
   "outputs": [],
   "source": [
    "CNN_trainer = Trainer(CNN_model, 5, train_dataloader, test_dataloader, device, CNN_criterion, CNN_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f8b488bdd222f85b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:29:50.453300800Z",
     "start_time": "2023-10-30T12:29:50.449223600Z"
    }
   },
   "outputs": [],
   "source": [
    "CNN_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bcba9d3eafb2cf40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:30:37.570766300Z",
     "start_time": "2023-10-30T12:29:50.453300800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:09<00:00, 14.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Loss: 3.305819, Train accuracy: 0.530848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:09<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5, Loss: 0.396682, Train accuracy: 0.823543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:09<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5, Loss: 0.095089, Train accuracy: 0.968289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:09<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5, Loss: 0.042491, Train accuracy: 0.987499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:09<00:00, 15.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5, Loss: 0.017543, Train accuracy: 0.996436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CNN_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "636f9382b19109da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:30:37.853360Z",
     "start_time": "2023-10-30T12:30:37.572766700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.052763, Validation accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "LSTM_model = LSTM(vocab_size=NUM_WORDS, embedding_dim=EMBED_DIM, hidden_dim=100, n_layers=3,\n",
    "                  seq_len=SENTENCE_LENGTH).to(device)\n",
    "LSTM_optimizer = torch.optim.Adam(LSTM_model.parameters(), lr=0.001)\n",
    "LSTM_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f2f785fd6f39615",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:30:37.998934900Z",
     "start_time": "2023-10-30T12:30:37.853360Z"
    }
   },
   "outputs": [],
   "source": [
    "LSTM_trainer = Trainer(LSTM_model, 2, train_dataloader, test_dataloader, device, LSTM_criterion, LSTM_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8fd81cb422cd7f40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:30:38.004482300Z",
     "start_time": "2023-10-30T12:30:37.998934900Z"
    }
   },
   "outputs": [],
   "source": [
    "LSTM_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "734709f28314625f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:30:45.340378600Z",
     "start_time": "2023-10-30T12:30:38.005471700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:03<00:00, 38.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2, Loss: 0.154895, Train accuracy: 0.926332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:03<00:00, 38.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2, Loss: 0.021886, Train accuracy: 0.992900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LSTM_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eb9f6c6420c41c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T12:30:45.662129200Z",
     "start_time": "2023-10-30T12:30:45.338382300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.039639, Validation accuracy: 0.987\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
